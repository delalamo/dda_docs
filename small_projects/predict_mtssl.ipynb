{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the distance between two amino acid residues spin labeled with methanethiosulfonate spin label (MTSSL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Diego del Alamo\n",
    "\n",
    "Date: 23 October 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the process of making and executing a very simple neural network to predict what some experimental data might look like. The experimental data, double electron-electron resonance (DEER) spectroscopy, measures the distance between two or more spin labels attached to a protein's amino acids. This \"molecular ruler\" allows scientists to observe distance changes as a protein responds to different conditions, substrates, etc. However, it is computationally expensive to predict from a known protein structure for comparison to experimental data.\n",
    "\n",
    "Here, I am attempting to quickly predict these distance data from a protein structure using machine learning. Whereas a normal framework might try to model the spin labels directly, here I am just using the spatial relationship between two residues as the inputs. This can be captured in six parameters: three rotational (Euler angles), and three translational (x, y, and z). The output is the single distance (note: in practice, DEER resturns a distribution. Here I am only calculating the average distance).\n",
    "\n",
    "Because there is very little experimental data to train this model, however, I first had to pre-trained it on about a million simulated distance values. Following this step, I import the experimental data and re-train the last layer of the neural network on the experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-67130a386b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I read in the simulated data, which is formatted in a CSV in a file called \"temp_data_230pm.csv\" as follows:\n",
    "Protein name, Residue 1, Residue 2, x, y, z, a, b, c, Average distance in Angstroms.\n",
    "\n",
    "In practice we are only interested in the last seven rows: rows 4-9 as the inputs, row 10 as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The header is not part of the file.\n",
    "header = [\"x\", \"y\", \"z\", \"a\", \"b\", \"g\", \"avg\"]\n",
    "# Import the data\n",
    "raw_data = pd.read_csv(\n",
    "    \"temp_data_230pm.csv\",\n",
    "    header=None,\n",
    "    names=header,\n",
    "    delimiter=\",\",\n",
    "    skip_blank_lines=True,\n",
    "    engine='python', # to mute warnings\n",
    "    usecols=range(3, 10),\n",
    "    skipfooter=1\n",
    ").sample(frac=1).reset_index(drop=True) # to randomize inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure used to simulate the data tends to make more short-distance data (<15 Angstroms). Although useful for training, we are experimentally more interested in longer distances, ranging from 20-80 Angstroms. Therefore, we should even things out if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of distance values in the csv file tend to skew short.\n",
    "for i in range( 5, 70 ): # Iterate over distance bins\n",
    "    print(i, raw_data[(raw_data[\"avg\"]>=i) & (raw_data[\"avg\"]<i+1.)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ac84bb889663>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mentries_per_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdist_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_dist\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mi_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# We will therefore make a new training set with a more even distribution of distance values\n",
    "min_dist = 5\n",
    "max_dist = 70\n",
    "entries_per_a = 10000\n",
    "\n",
    "dist_data = pd.DataFrame()\n",
    "for i in range(min_dist, max_dist+1):\n",
    "    i_data = raw_data[(raw_data[\"avg\"]>i) & (raw_data[\"avg\"]<i+1.)]\n",
    "    dist_data = pd.concat([dist_data, i_data.head(entries_per_a)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now these data must be split into a training set and a test set. I like to take 20% of the data aside for validation purposes. The training data are then placed in a PyTorch DataLoader container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_train = 0.8\n",
    "ids = np.random.rand( len( dist_data ) ) < pct_train\n",
    "\n",
    "train_data, test_data = dist_data[ ids ], dist_data[ ~ids ]\n",
    "\n",
    "train_x = torch.from_numpy( train_data[[ \"x\", \"y\", \"z\", \"a\", \"b\", \"g\" ]].values ).float()\n",
    "train_y = torch.from_numpy( train_data[[ \"avg\" ]].values ).float()\n",
    "\n",
    "test_x = torch.from_numpy( test_data[[ \"x\", \"y\", \"z\", \"a\", \"b\", \"g\" ]].values ).float()\n",
    "test_y = torch.from_numpy( test_data[[ \"avg\" ]].values ).float()\n",
    "\n",
    "# Package it up in a PyTorch container\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=torch.utils.data.TensorDataset( train_x, train_y ),\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must define our neural network. Since we are expecting a relatively simple relationship between these six degrees of freedom and the output, I would prefer to make the network as shallow as possible. Five layers should suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7fa3256bf239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mD_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model = torch.nn.Sequential( OrderedDict( [\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m(\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mD_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m(\u001b[0m \u001b[0;34m'r1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "D_in = 6\n",
    "H = 6\n",
    "D_out = 1\n",
    "\n",
    "model = torch.nn.Sequential( OrderedDict( [\n",
    "    ( 'l1', torch.nn.Linear( D_in, H ) ),\n",
    "    ( 'r1', torch.nn.ReLU() ),\n",
    "    ( 'l2', torch.nn.Linear( H, H ) ),\n",
    "    ( 'r2', torch.nn.ReLU() ),\n",
    "    ( 'l3', torch.nn.Linear( H, H ) ),\n",
    "    ( 'r3', torch.nn.ReLU() ),\n",
    "    ( 'l4', torch.nn.Linear( H, H ) ),\n",
    "    ( 'r4', torch.nn.ReLU() ),\n",
    "    ( 'l5', torch.nn.Linear( H, D_out ) ),\n",
    "    ( 'r5', torch.nn.ReLU() )\n",
    "] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we define some parameters relating to the training of this network during the first round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ee7ca468b721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fxn = torch.nn.MSELoss()\n",
    "\n",
    "learn_rate = 3e-4\n",
    "\n",
    "optimizer = torch.optim.Adam( model.parameters(), lr=learn_rate )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the function that will actually train the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model( model, train_loader, optimizer, loss_fxn ):\n",
    "    model.train()\n",
    "    for n, ( x_batch, y_batch ) in enumerate( train_loader ):\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fxn( model( x_batch ), y_batch )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and here we define some functions that will evaluate the model during each epoch following training and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model( model, test_x, test_y, n_bins_eval ):\n",
    "    model.eval()\n",
    "    eval_y = model( test_x )\n",
    "    diff = torch.abs( val_pred - test_y ).detach().numpy()\n",
    "    score_val = np.sum( diff_vec / len( eval_y ) )\n",
    "    eval_bins = [100.*len(diff_vec[diff_vec<=n])/len(test_y) for n in range(n_bins_eval+1)]\n",
    "    return eval_bins\n",
    "\n",
    "def print_bins( eval_bins ):\n",
    "    print( \"{:5d}\\t{:22.4f}\", end=\"\\t\" )\n",
    "    for n in range( len( eval_bins ) ):\n",
    "        print( eval_bins[ n ], end=\"\\t\" )\n",
    "    print( \"\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will start training this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tAvg. Dev. (Validation)\t1.0 A\t2.0 A\t3.0 A\t4.0 A\t5.0 A\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-26836faa965b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mn_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fxn\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint_bins\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_bins\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the headers\n",
    "print( \"Epoch\", end=\"\\t\" )\n",
    "print( \"Avg. Dev. (Validation)\", end=\"\\t\" )\n",
    "print( \"1.0 A\", end=\"\\t\" )\n",
    "print( \"2.0 A\", end=\"\\t\" )\n",
    "print( \"3.0 A\", end=\"\\t\" )\n",
    "print( \"4.0 A\", end=\"\\t\" )\n",
    "print( \"5.0 A\" )\n",
    "\n",
    "# I found this to be a good number of runs, there does not appear to be any benefit\n",
    "n_epochs = 25\n",
    "n_bins = 5\n",
    "for epoch in range( n_epochs ):\n",
    "    train_model( model, train_loader, optimizer, loss_fxn )\n",
    "    print_bins( eval_model( model, test_x, test_y, n_bins ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming everything went well, we now have a model capable of predicting the distance between two residues. However, this was trained on simulated data, not experimental data. Experimental data is more difficult to predict for many reasons. We therefore need to refine our model on the data we have, which is not much. First, we need to import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a671f1ff5a3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m raw_data2 = pd.read_csv(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"experimental_data_17oct2020.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "raw_data2 = pd.read_csv(\n",
    "    \"experimental_data_17oct2020.csv\",\n",
    "    header=None,\n",
    "    names=header,\n",
    "    delimiter=\",\",\n",
    "    skip_blank_lines=True,\n",
    "    engine='python', # to mute warnings\n",
    "    usecols=range(3, 10),\n",
    "    skipfooter=1\n",
    ").sample(frac=1).reset_index(drop=True) # to randomize inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
